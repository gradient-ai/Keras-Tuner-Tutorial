{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# install required packages\n",
    "!pip install tensorflow\n",
    "!pip install keras_tuner\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2699056429.py, line 2)",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [1]\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install tensorflow\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import required packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D, BatchNormalization\n",
    "from tensorflow.keras.layers import ReLU, MaxPool2D, AvgPool2D, GlobalAvgPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import keras_tuner as kt\n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load the CIFAR-10 dataset from keras\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "\n",
    "# Normalize the image pixel values\n",
    "img_train = x_train.astype('float32') / 255.0\n",
    "img_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# split the train data into train and validation sets\n",
    "x_train, y_train, x_val, y_val = train_test_split(\n",
    "    x_train, y_train,                                                             test_size=0.25)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# function to build an hypermodel\n",
    "# takes an argument from which to sample hyperparameters\n",
    "def build_model(hp):\n",
    "  inputs = Input(shape=(32, 32, 3))  # input layer\n",
    "  x = inputs\n",
    "  # iterate a number of conv blocks from min_value to max_value\n",
    "  # tune the number of filters\n",
    "  # choose an optimal value from min_value to max_value\n",
    "  # Int specifies the dtype of the values\n",
    "  for i in range(hp.Int('conv_blocks', min_value=3, max_value=5, default=3)):\n",
    "    filters = hp.Int('filters_' + str(i), min_value=32, max_value=256, step=32)\n",
    "    for _ in range(2):\n",
    "      # define the conv, BatchNorm and activation layers for each block\n",
    "      x = Convolution2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "      x = BatchNormalization()(x)\n",
    "      x = ReLU()(x)\n",
    "    # choose an optimal pooling type\n",
    "    # hp.Choice chooses from a list of values\n",
    "    if hp.Choice('pooling_' + str(i), ['avg', 'max']) == 'max':\n",
    "        x = MaxPool2D()(x)\n",
    "    else:\n",
    "        x = AvgPool2D()(x)\n",
    "  x = GlobalAvgPool2D()(x)  # apply GlobalAvG Pooling\n",
    "  # Tune the number of units in the  Dense layer\n",
    "  # Choose an optimal value between min_value to max_value\n",
    "  x = Dense(hp.Int('Dense units', min_value=30, max_value=100,\n",
    "            step=10, default=50), activation='relu')(x)\n",
    "  outputs = Dense(10, activation='softmax')(x)  # output layer\n",
    "\n",
    "  # define the model\n",
    "  model = Model(inputs, outputs)\n",
    "  # Tune the learning rate for the optimizer\n",
    "  # Choose an optimal value frommin_value to max_value\n",
    "  model.compile(optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')),\n",
    "                loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "  return model\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+200B (2873801852.py, line 4)",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [3]\u001b[0;36m\u001b[0m\n\u001b[0;31m    â€‹\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid non-printable character U+200B\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# initialize tuner to run the model.\n",
    "# using the Hyperband search algorithm\n",
    "tuner = kt.Hyperband(\n",
    "    hypermodel=build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=30,\n",
    "    hyperband_iterations=2,\n",
    "    directory=\"Keras_tuner_dir\",\n",
    "    project_name=\"Keras_tuner_Demo\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize a random search tuner\n",
    "# using the Resnet architecture\n",
    "# and the Random Search algorithm\n",
    "tuner = kt.tuners.RandomSearch(\n",
    "    kt.applications.HyperResNet(input_shape=(32, 32, 3), classes=10),\n",
    "    objective='val_accuracy',\n",
    "    max_trials=30)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run the search\n",
    "tuner.search(x_train, y_train,\n",
    "             validation_data=(x_test, y_test),\n",
    "             epochs=30,\n",
    "             callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# get the best model\n",
    "best_model = tuner.get_best_models(1)[0]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nblocks = best_hps.get('conv_blocks')\n",
    "print(f'Number of conv blocks: {nblocks}')\n",
    "for hyparam in [f'filters_{i}' for i in range(nblocks)] + [f'pooling_{i}' for i in range(nblocks)] + ['Dense units'] + ['learning_rate']:\n",
    "    print(f'{hyparam}: {best_hps.get(hyparam)}')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# display model structure\n",
    "plot_model(best_model, 'best_model.png', show_shapes=True)\n",
    "\n",
    "# show model summary\n",
    "best_model.summary()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Build the model with the optimal hyperparameters\n",
    "# train the model.\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.fit(x_train, y_train, \n",
    "          validation_data= (x_val,y_val), \n",
    "          epochs= 25,\n",
    "           callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate the result"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# evaluate the result\n",
    "eval_result = model.evaluate(x_test, y_test)\n",
    "print(f\"test loss: {eval_result[0]}, test accuracy: {eval_result[1]}\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.13 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}